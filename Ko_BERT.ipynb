{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOki9iMkYarNkKCzZcUJAc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wookddang/PythonApplication2/blob/master/Ko_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install SpeechRecognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-waQUkJ3seAh",
        "outputId": "65ff2456-c962-47d2-ec5d-6373f455816d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2024.2.2)\n",
            "Installing collected packages: SpeechRecognition\n",
            "Successfully installed SpeechRecognition-3.10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "F86ytqoWikL5",
        "outputId": "87415eab-81ee-49d0-af3b-dc5806653923"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-f55118b380db>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f55118b380db>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install SpeechRecognition\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install speechrecognition kobert-transformers transformers tensorflow-addons tqdm seaborn matplotlib scikit-learn"
      ],
      "metadata": {
        "id": "hRA65iGOi1Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "qSarCEe9ivVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import TFBertModel\n",
        "import tensorflow_addons as tfa\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# KoBERT Setup\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "model = TFBertModel.from_pretrained('skt/kobert-base-v1', from_pt=True)\n",
        "\n",
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 2\n",
        "LR = 1e-5\n",
        "\n",
        "# Data Loading Functions\n",
        "def convert_data(data_df):\n",
        "    global tokenizer\n",
        "    indices, targets = [], []\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        tokenized = tokenizer(data_df.iloc[i][\"comment\"], padding='max_length', max_length=SEQ_LEN, truncation=True, return_tensors=\"tf\")\n",
        "        indices.append(tokenized['input_ids'][0].numpy())\n",
        "        targets.append(data_df.iloc[i][\"label\"])\n",
        "    indices = np.array(indices)\n",
        "    return [indices, np.zeros_like(indices)], np.array(targets)\n",
        "\n",
        "def load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[\"comment\"] = data_df[\"comment\"].astype(str)\n",
        "    data_x, data_y = convert_data(data_df)\n",
        "    return data_x, data_y\n",
        "\n",
        "# Load the data\n",
        "#!git clone https://github.com/e9t/nsmc.git || true\n",
        "train = pd.read_csv(\"/content/train_set2.csv\", encoding='cp949')\n",
        "test = pd.read_csv(\"/content/test_set2.csv\", encoding='cp949')\n",
        "\n",
        "train_x, train_y = load_data(train)\n",
        "test_x, test_y = load_data(test)\n",
        "\n",
        "# Define the model architecture\n",
        "class SentimentClassifier(tf.keras.Model):\n",
        "    def __init__(self, bert):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        input_ids = inputs[0]\n",
        "        attention_mask = tf.cast(tf.math.not_equal(input_ids, 0), tf.int32)\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "        cls_output = self.classifier(cls_output)\n",
        "        return cls_output\n",
        "\n",
        "kobert_model = TFBertModel.from_pretrained('skt/kobert-base-v1', from_pt=True)\n",
        "model = SentimentClassifier(kobert_model)\n",
        "optimizer = tfa.optimizers.AdamW(learning_rate=LR, weight_decay=1e-5)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_x, train_y, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, validation_data=(test_x, test_y), shuffle=True)\n",
        "\n",
        "# Save the model weights\n",
        "model.save_weights(\"kobert_model.h5\")\n",
        "\n",
        "# Load the model weights\n",
        "model.load_weights(\"kobert_model.h5\")\n",
        "\n",
        "# Predictions\n",
        "def predict_load_data(x):\n",
        "    data_df = x\n",
        "    data_df[\"comment\"] = data_df[\"comment\"].astype(str)\n",
        "    data_x, data_y = convert_data(data_df)\n",
        "    return data_x\n",
        "\n",
        "test_set = predict_load_data(test)\n",
        "\n",
        "# Prediction\n",
        "preds = model.predict(test_set)\n",
        "\n",
        "# F1 Score 확인\n",
        "y_true = test['label']\n",
        "print(classification_report(y_true, np.round(preds, 0)))\n",
        "\n",
        "def sentence_convert_data(data):\n",
        "    global tokenizer\n",
        "    indices = []\n",
        "    tokenized = tokenizer(data, padding='max_length', max_length=SEQ_LEN, truncation=True, return_tensors=\"tf\")\n",
        "    indices.append(tokenized['input_ids'][0].numpy())\n",
        "    indices = np.array(indices)\n",
        "    return [indices, np.zeros_like(indices)]\n",
        "\n",
        "def movie_evaluation_predict(sentence):\n",
        "    data_x = sentence_convert_data(sentence)\n",
        "    predict = model.predict(data_x)\n",
        "    predict_answer = np.round(np.ravel(predict), 0).item()\n",
        "    if predict_answer == 0:\n",
        "        print(\"보이스피싱입니다.\")\n",
        "    elif predict_answer == 1:\n",
        "        print(\"보이스피싱이 아닙니다\")\n"
      ],
      "metadata": {
        "id": "tReSJdOcjNTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_evaluation_predict(\" 집과 직장에서 인터넷 뱅킹을 이용가능하나요? 공인인증서가 있습니까? 네 가능합니다. 공인인증서를 복사해서 사용할 수 있습니다. 인증서 복사는 어떻게 하죠? 하드디스크에서 이동식USB로 복사하는 방법이 있습니다. 이동식 USB말고 다른 방법있나요? 기기간에 복사할 수 있는 방법이 있는데 안내해 드릴까요? 네 인증서 내보내기 후 암호를 입력하고 8자리 인증번호를 확인해 주세요 인증서를 가져올 기기에서는 어떻게 해야하나요? 인증서 가저오기를 누르시고 8자리 인증번호입력후 저장위치를 선택한후 암호를 입력하시면 됩니다. 이메일로도 가능한가요? 가능합니다. 어떻게 해야하나요? 인증서를 내보내기 후 이메일로 받은 인증서 파일을 바탕화면에 저장한 후 인증서 가져오기 누르시면 됩니다. 가정  직장 말고도 노트북에도 가능한가요? 네 같은 방법으로 하면 되나요? 네 동일한 방법으로 하시면 됩니다.\")"
      ],
      "metadata": {
        "id": "AKsVRyx7aOO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}