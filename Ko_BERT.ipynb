{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7PDWtgPTbZe5OZCpgSZw0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wookddang/PythonApplication2/blob/master/Ko_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F86ytqoWikL5"
      },
      "outputs": [],
      "source": [
        "pip install SpeechRecognition\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install speechrecognition kobert-transformers transformers tensorflow-addons tqdm seaborn matplotlib scikit-learn"
      ],
      "metadata": {
        "id": "hRA65iGOi1Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup"
      ],
      "metadata": {
        "id": "qSarCEe9ivVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import TFBertModel\n",
        "import tensorflow_addons as tfa\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "\n",
        "warnings.filterwarnings(action='ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# KoBERT Setup\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "model = TFBertModel.from_pretrained('skt/kobert-base-v1', from_pt=True)\n",
        "\n",
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 2\n",
        "LR = 1e-5\n",
        "\n",
        "# Data Loading Functions\n",
        "def convert_data(data_df):\n",
        "    global tokenizer\n",
        "    indices, targets = [], []\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        tokenized = tokenizer(data_df.iloc[i][\"comment\"], padding='max_length', max_length=SEQ_LEN, truncation=True, return_tensors=\"tf\")\n",
        "        indices.append(tokenized['input_ids'][0].numpy())\n",
        "        targets.append(data_df.iloc[i][\"label\"])\n",
        "    indices = np.array(indices)\n",
        "    return [indices, np.zeros_like(indices)], np.array(targets)\n",
        "\n",
        "def load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    data_df[\"comment\"] = data_df[\"comment\"].astype(str)\n",
        "    data_x, data_y = convert_data(data_df)\n",
        "    return data_x, data_y\n",
        "\n",
        "# Load the data\n",
        "#!git clone https://github.com/e9t/nsmc.git || true\n",
        "train = pd.read_table(\"/content/train_set2.csv\", encoding='cp949')\n",
        "test = pd.read_table(\"/content/test_set2.csv\", encoding='cp949')\n",
        "\n",
        "# Use only 500 samples\n",
        "train = train.sample(n=500, random_state=42)\n",
        "test = test.sample(n=500, random_state=42)\n",
        "\n",
        "train_x, train_y = load_data(train)\n",
        "test_x, test_y = load_data(test)\n",
        "\n",
        "# Define the model architecture\n",
        "class SentimentClassifier(tf.keras.Model):\n",
        "    def __init__(self, bert):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.bert = bert\n",
        "        self.classifier = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        input_ids = inputs[0]\n",
        "        attention_mask = tf.cast(tf.math.not_equal(input_ids, 0), tf.int32)\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "        cls_output = self.classifier(cls_output)\n",
        "        return cls_output\n",
        "\n",
        "kobert_model = TFBertModel.from_pretrained('skt/kobert-base-v1', from_pt=True)\n",
        "model = SentimentClassifier(kobert_model)\n",
        "optimizer = tfa.optimizers.AdamW(learning_rate=LR, weight_decay=1e-5)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_x, train_y, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, validation_data=(test_x, test_y), shuffle=True)\n",
        "\n",
        "# Save the model weights\n",
        "model.save_weights(\"kobert_model.h5\")\n",
        "\n",
        "# Load the model weights\n",
        "model.load_weights(\"kobert_model.h5\")\n",
        "\n",
        "# Predictions\n",
        "def predict_load_data(x):\n",
        "    data_df = x\n",
        "    data_df[\"comment\"] = data_df[\"comment\"].astype(str)\n",
        "    data_x, data_y = convert_data(data_df)\n",
        "    return data_x\n",
        "\n",
        "test_set = predict_load_data(test)\n",
        "\n",
        "# Prediction\n",
        "preds = model.predict(test_set)\n",
        "\n",
        "# F1 Score 확인\n",
        "y_true = test['label']\n",
        "print(classification_report(y_true, np.round(preds, 0)))\n",
        "\n",
        "def sentence_convert_data(data):\n",
        "    global tokenizer\n",
        "    indices = []\n",
        "    tokenized = tokenizer(data, padding='max_length', max_length=SEQ_LEN, truncation=True, return_tensors=\"tf\")\n",
        "    indices.append(tokenized['input_ids'][0].numpy())\n",
        "    indices = np.array(indices)\n",
        "    return [indices, np.zeros_like(indices)]\n",
        "\n",
        "def movie_evaluation_predict(sentence):\n",
        "    data_x = sentence_convert_data(sentence)\n",
        "    predict = model.predict(data_x)\n",
        "    predict_answer = np.round(np.ravel(predict), 0).item()\n",
        "    if predict_answer == 0:\n",
        "        print(\"보이스피싱입니다.\")\n",
        "    elif predict_answer == 1:\n",
        "        print(\"보이스피싱이 아닙니다\n",
        "movie_evaluation_predict(\"서울중앙지검으로 송금하세요 \")"
      ],
      "metadata": {
        "id": "tReSJdOcjNTe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}